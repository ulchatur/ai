name: Job cluster creation1

on:
  workflow_dispatch:
    inputs:
      job-name:
        description: 'Job Name'
        type: string
        default: 'weekly-scheduled-job'
      notebook-path:
        description: 'Notebook Path in Workspace'
        type: string
        default: '/Users/vardhanullas7@gmail.com/hello-world'
      spark-version:
        description: 'Spark Version'
        type: string
        default: '15.2.x-scala2.12'
      node-type-id:
        description: 'Node Type ID'
        type: string
        default: 'Standard_DS3_v2'
      num-workers:
        description: 'Number of Workers'
        type: string
        default: '1'

jobs:
  create-job:
    runs-on: ubuntu-latest
    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Install Databricks CLI v0.18+
        uses: databricks/setup-cli@main

      - name: Create Databricks Job with Cluster & Schedule (Multi-task format)
        id: create-job
        run: |
          job_json=$(cat <<EOF
          {
            "name": "${{ github.event.inputs.job-name }}",
            "tasks": [
              {
                "task_key": "hello-task",
                "notebook_task": {
                  "notebook_path": "${{ github.event.inputs.notebook-path }}"
                },
                "new_cluster": {
                  "spark_version": "${{ github.event.inputs.spark-version }}",
                  "node_type_id": "${{ github.event.inputs.node-type-id }}",
                  "num_workers": ${{ github.event.inputs.num-workers }}
                }
              }
            ],
            "schedule": {
              "quartz_cron_expression": "0 0 10 ? * MON",
              "timezone_id": "Asia/Kolkata",
              "pause_status": "UNPAUSED"
            },
            "max_concurrent_runs": 1
          }
          EOF
          )

          echo "$job_json" > job.json
          echo "Creating Databricks Job..."
          job_response=$(databricks jobs create --json @job.json)
          echo "$job_response" > job_response.json
          job_id=$(echo "$job_response" | jq -r '.job_id')
          echo "job-id=$job_id" >> $GITHUB_OUTPUT

      - name: Print Job and Cluster Details
        run: |
          echo "âœ… Job Created Successfully!"
          echo "ðŸ†” Job ID        : ${{ steps.create-job.outputs.job-id }}"
          echo "ðŸ“› Job Name      : ${{ github.event.inputs.job-name }}"
